{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c525f2",
   "metadata": {},
   "source": [
    "# This project aims to build a computer vision deep learning model using convolutional neural network with open source data downloaded from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564ace32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77188c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our constants\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32  \n",
    "Epochs = 50\n",
    "CHANNELS = 3\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory( # importing our image data into tensorflow\n",
    "    \"PlantVillage\",\n",
    "    batch_size= BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,   # shuffling our data so to prevent bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "946c2751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = dataset.class_names\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9550e",
   "metadata": {},
   "source": [
    "### 80/10/10 Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9c910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    ds_size = len(ds)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size)\n",
    "    train_ds = ds.take(int(len(dataset)*train_split))\n",
    "    val_ds = ds.skip(int(len(dataset)*train_split)).take(int(len(ds)*val_split))\n",
    "    test_ds = ds.skip(int(len(dataset)*train_split)).skip(int(len(ds)*val_split))\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7a7dd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 6, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = train_val_test_split(dataset)\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52171c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226acc1",
   "metadata": {},
   "source": [
    "### Data Augmentation to add more data to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9ce1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ed55997",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e4c7567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_3 (Sequential)   (256, 256, 256)           0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (32, 254, 254, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_36 (MaxPooli  (32, 127, 127, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (32, 125, 125, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_37 (MaxPooli  (32, 62, 62, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (32, 60, 60, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_38 (MaxPooli  (32, 30, 30, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (32, 28, 28, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPooli  (32, 14, 14, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (32, 12, 12, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPooli  (32, 6, 6, 64)            0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (32, 4, 4, 64)            36928     \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPooli  (32, 2, 2, 64)            0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (32, 256)                 0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (32, 64)                  16448     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (32, 3)                   195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183747 (717.76 KB)\n",
      "Trainable params: 183747 (717.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # 6 rounds of convolution and max pooling layers before dense layer\n",
    "    # trials and error \n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f85c3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a1bb3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "54/54 [==============================] - 117s 2s/step - loss: 0.8617 - accuracy: 0.5053 - val_loss: 0.7217 - val_accuracy: 0.5938\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 79s 1s/step - loss: 0.6342 - accuracy: 0.6937 - val_loss: 0.5022 - val_accuracy: 0.7552\n",
      "Epoch 3/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.3873 - accuracy: 0.8363 - val_loss: 0.2272 - val_accuracy: 0.9115\n",
      "Epoch 4/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.2967 - accuracy: 0.8744 - val_loss: 0.1341 - val_accuracy: 0.9583\n",
      "Epoch 5/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.1921 - accuracy: 0.9302 - val_loss: 0.1228 - val_accuracy: 0.9375\n",
      "Epoch 6/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.1429 - accuracy: 0.9396 - val_loss: 0.0760 - val_accuracy: 0.9740\n",
      "Epoch 7/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0940 - accuracy: 0.9665 - val_loss: 0.0975 - val_accuracy: 0.9531\n",
      "Epoch 8/50\n",
      "54/54 [==============================] - 79s 1s/step - loss: 0.0889 - accuracy: 0.9683 - val_loss: 0.0643 - val_accuracy: 0.9792\n",
      "Epoch 9/50\n",
      "54/54 [==============================] - 201s 4s/step - loss: 0.0676 - accuracy: 0.9748 - val_loss: 0.0687 - val_accuracy: 0.9740\n",
      "Epoch 10/50\n",
      "54/54 [==============================] - 209s 4s/step - loss: 0.0427 - accuracy: 0.9853 - val_loss: 0.0382 - val_accuracy: 0.9844\n",
      "Epoch 11/50\n",
      "54/54 [==============================] - 235s 4s/step - loss: 0.0795 - accuracy: 0.9654 - val_loss: 0.0872 - val_accuracy: 0.9688\n",
      "Epoch 12/50\n",
      "54/54 [==============================] - 241s 4s/step - loss: 0.0634 - accuracy: 0.9777 - val_loss: 0.0286 - val_accuracy: 0.9844\n",
      "Epoch 13/50\n",
      "54/54 [==============================] - 235s 4s/step - loss: 0.0841 - accuracy: 0.9683 - val_loss: 0.0352 - val_accuracy: 0.9896\n",
      "Epoch 14/50\n",
      "54/54 [==============================] - 199s 4s/step - loss: 0.0239 - accuracy: 0.9906 - val_loss: 0.0458 - val_accuracy: 0.9792\n",
      "Epoch 15/50\n",
      "54/54 [==============================] - 209s 4s/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 0.0210 - val_accuracy: 0.9896\n",
      "Epoch 16/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0207 - val_accuracy: 0.9896\n",
      "Epoch 17/50\n",
      "54/54 [==============================] - 169s 3s/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.0262 - val_accuracy: 0.9896\n",
      "Epoch 18/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0054 - accuracy: 0.9977 - val_loss: 0.0349 - val_accuracy: 0.9948\n",
      "Epoch 19/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0646 - accuracy: 0.9824 - val_loss: 0.0910 - val_accuracy: 0.9688\n",
      "Epoch 20/50\n",
      "54/54 [==============================] - 105s 2s/step - loss: 0.0495 - accuracy: 0.9806 - val_loss: 0.0442 - val_accuracy: 0.9948\n",
      "Epoch 21/50\n",
      "54/54 [==============================] - 232s 4s/step - loss: 0.0114 - accuracy: 0.9959 - val_loss: 0.0323 - val_accuracy: 0.9844\n",
      "Epoch 22/50\n",
      "54/54 [==============================] - 395s 7s/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 0.0048 - val_accuracy: 0.9948\n",
      "Epoch 23/50\n",
      "54/54 [==============================] - 116s 2s/step - loss: 0.0090 - accuracy: 0.9953 - val_loss: 0.0555 - val_accuracy: 0.9844\n",
      "Epoch 24/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0137 - accuracy: 0.9953 - val_loss: 0.0235 - val_accuracy: 0.9896\n",
      "Epoch 25/50\n",
      "54/54 [==============================] - 79s 1s/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0170 - val_accuracy: 0.9896\n",
      "Epoch 26/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 7.6631e-04 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 0.9948\n",
      "Epoch 27/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 1.6060e-04 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 0.9896\n",
      "Epoch 28/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 1.2034e-04 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 0.9896\n",
      "Epoch 29/50\n",
      "54/54 [==============================] - 374s 7s/step - loss: 9.8378e-05 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 0.9948\n",
      "Epoch 30/50\n",
      "54/54 [==============================] - 134s 3s/step - loss: 8.2450e-05 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 0.9948\n",
      "Epoch 31/50\n",
      "54/54 [==============================] - 231s 4s/step - loss: 7.2070e-05 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 0.9948\n",
      "Epoch 32/50\n",
      "54/54 [==============================] - 213s 4s/step - loss: 6.3701e-05 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 0.9948\n",
      "Epoch 33/50\n",
      "54/54 [==============================] - 264s 5s/step - loss: 5.6074e-05 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 0.9948\n",
      "Epoch 34/50\n",
      "54/54 [==============================] - 2400s 45s/step - loss: 5.1126e-05 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 0.9948\n",
      "Epoch 35/50\n",
      "54/54 [==============================] - 250s 5s/step - loss: 4.5829e-05 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 0.9948\n",
      "Epoch 36/50\n",
      "54/54 [==============================] - 854s 16s/step - loss: 4.1736e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9948\n",
      "Epoch 37/50\n",
      "54/54 [==============================] - 225s 4s/step - loss: 3.8182e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9948\n",
      "Epoch 38/50\n",
      "54/54 [==============================] - 260s 5s/step - loss: 3.4966e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9948\n",
      "Epoch 39/50\n",
      "54/54 [==============================] - 15445s 291s/step - loss: 3.2415e-05 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 0.9948\n",
      "Epoch 40/50\n",
      "54/54 [==============================] - 72s 1s/step - loss: 2.9649e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9948\n",
      "Epoch 41/50\n",
      "54/54 [==============================] - 76s 1s/step - loss: 2.7675e-05 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 0.9948\n",
      "Epoch 42/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 2.5735e-05 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 0.9948\n",
      "Epoch 43/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 2.3836e-05 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 0.9948\n",
      "Epoch 44/50\n",
      "54/54 [==============================] - 82s 2s/step - loss: 2.2490e-05 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 0.9948\n",
      "Epoch 45/50\n",
      "54/54 [==============================] - 79s 1s/step - loss: 2.0916e-05 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 0.9948\n",
      "Epoch 46/50\n",
      "54/54 [==============================] - 75s 1s/step - loss: 1.9598e-05 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 0.9948\n",
      "Epoch 47/50\n",
      "54/54 [==============================] - 74s 1s/step - loss: 1.8574e-05 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 0.9948\n",
      "Epoch 48/50\n",
      "54/54 [==============================] - 75s 1s/step - loss: 1.7472e-05 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9948\n",
      "Epoch 49/50\n",
      "54/54 [==============================] - 75s 1s/step - loss: 1.6358e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9948\n",
      "Epoch 50/50\n",
      "54/54 [==============================] - 75s 1s/step - loss: 1.5403e-05 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 0.9948\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1a50d",
   "metadata": {},
   "source": [
    "Validation loss is around 1% and validation accuracy is more than 99%. Our model is quite accurate at classifying novel potatoes disease images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "650f19e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 199ms/step - loss: 0.0090 - accuracy: 0.9961\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93b74f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009006748907268047, 0.99609375]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94565371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/3\\assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_version=max([int(i) for i in os.listdir(\"../models\") + [0]])+1\n",
    "model.save(f\"../models/{model_version}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
